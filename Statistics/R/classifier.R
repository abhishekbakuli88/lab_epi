## author: Peter PÃ¼tz, peter.puetz@uni-bielefeld.de

# This script creates the output table 1 in the lab paper (Olbrich et al., 2020)

# specify folders for script/data/output loading and saving
here_r = function (...)
  here::here("Statistics", "R", ...)
here_lab_data = function (...)
  here::here("Lab_paper2_data", ...)
here_algo_results = function (...)
  here::here("AlgorithmResults", ...)
here_out = function (...)
  here::here("Lab_paper2_output", "classifier", ...)
here_sampling_weights = function (...)
  here::here("SamplingWeights", ...)

# Create output folder if inexistent
dir.create(here_out(), showWarnings = FALSE)

# Load settings and functions
source(here_r("setup.R"))
source(here_r("functions.R"))

# Specify the list of required packages to be installed and load
Required_Packages <-
  c("tidyverse",
    "reshape2",
    "ggpubr",
    "flextable",
    "magrittr",
    "svglite")

# install (if necessary) and load packages
Install_And_Load(Required_Packages)

# Load data
#  Single data contains only one blood sample per participant
data = read.csv(here_lab_data("Global_Data", "Final_Lab_Single_20200904.csv"),
                stringsAsFactors = F) %>%
  as_tibble()

#  Unique labels
data[data == "Positiv" |
       data == "pos." | data == "reactive" |
       data == "positive"] = "positive"
data[data == "Negativ" |
       data == "Indeterminate" | data == "neg." | data == "ind." |
       data == "nonreactive" | data == "negative"] = "negative"

#  Remove false Roche 0 values (these are NAs)
val_cols = grep("roche_COI", colnames(data), value = T)
res_cols = grep("roche_Interpretation", colnames(data), value = T)

for (j in 1:length(val_cols)) {
  # Conveniently, the order is the same
  val_col = val_cols[[j]]
  res_col = res_cols[[j]]
  data[data[, val_col] == 0 & !is.na(data[, val_col]), res_col] = NA
  data[data[, val_col] == 0 & !is.na(data[, val_col]), val_col] = NA
}

# Extract last values for single tests
data = extract_last_values(data) %>% dplyr::rename("iga" = "Eur_IgA",
                                                   "igg" = "Eur_IgG",
                                                   "roche" = "Roche")

## performance criteria such as specificity for single tests based on
## manufacturers' cutoffs and for random forest
## and support vector machine (svm)
# either run respective script (takes some time)...
# source(here_r("algorithm_manucutoff_rf_svm.R"))
# ...or load data that is generated by this script, i.e.
# - performance criteria
results_provided_cutoffs <-
  readRDS(here_algo_results("results_provided_cutoffs.RData"))
results_rf <- readRDS(here_algo_results("results_rf.RData"))
results_svm <- readRDS(here_algo_results("results_svm.RData"))
# - "optimal" random forest and svm, will be used for predictions for cohort data
rf <- readRDS(here_algo_results("rf_optimal.RData"))
svmfit <- readRDS(here_algo_results("svm_optimal.RData"))

## optimised cut-offs and performance criteria such as specificity for single tests
## with optimized cutoffs
# either run respective script (takes some time)...
# source(here_r("algorithm_optcutoff.R"))
# ...or load data that is generated by this script, i.e.
# - performance criteria
results_opt_cutoffs <-
  readRDS(here_algo_results("perf_measures_primary.RData"))
results_opt_cutoffs_conf <-
  readRDS(here_algo_results("perf_measures_confirmatory.RData"))
# - optimized cutoffs
cutoffs_all_tests <-
  readRDS(here_algo_results("cutoffs.RData"))


# Performance measures for all classifiers -----------------------------------------
# combine performance measures
perf_measures <-
  cbind(results_opt_cutoffs, results_opt_cutoffs_conf) %>%
  # and transpose them
  t()

# select specificities, sensitivities and overall accuracy
perf_measures <- perf_measures[, 2:4]

# assign names to the performance criteria
colnames(perf_measures) <-
  c("Specificity", "Sensitivity", "Overall accuracy")

# combine with sample sizes and optimized cut-off
perf_measures %<>%
  as_tibble() %>%
  bind_cols(cutoffs_all_tests) %>%
  # select relevant columns
  dplyr::select(positives,
                negatives,
                Median,
                Specificity,
                Sensitivity,
                `Overall accuracy`) %>%
  # add Test variable
  add_column(Test = rownames(cutoffs_all_tests))


# transpose cutoffs and performance measures for manufacturers' cutoffs
results_provided_cutoffs_t <- results_provided_cutoffs %>%
  as_tibble() %>%
  t()

# assign colnames
colnames(results_provided_cutoffs_t) <-
  c(
    "Manufacturer's cut-off",
    "Spec. for Manu. cut-off",
    "Sens. for Manu. cut-off",
    "Ov. accur. for Manu cut-off",
    "au_roc"
  )

results_provided_cutoffs_final <- results_provided_cutoffs_t %>%
  as_tibble() %>%
  # add test variable
  add_column(Test = rownames(results_provided_cutoffs_t)) %>%
  # drop area under ROC
  select(-au_roc)

# add manufacturers' cutoffs and performance measures to table
perf_measure_table <-
  left_join(perf_measures, results_provided_cutoffs_final) %>%
  # drop some of the arrays
  dplyr::filter(!grepl("_IgA", Test),!grepl("_IgM", Test))

# add random forest and svm
ml_clas <- rbind(results_rf, results_svm)[, 1:4] %>%
  as_tibble()

# make colnames compatible
colnames(ml_clas) <-
  c("Median", "Specificity", "Sensitivity", "Overall accuracy")

# add sample sizes and make table compatible
ml_clas %<>%
  add_column(
    positives = c(193, 193),
    negatives = c(1073, 1073),
    Test = c("Random Forest", "SVM")
  )

# join with other classifiers
perf_measure_table %<>%
  bind_rows(ml_clas) %>%
  # reorder rows
  arrange(match(Test, c("iga", "igg", "roche"))) %>%
  # set to 3 decimals
  mutate_if(is.numeric,  ~ sprintf("%.4f", .))

# recode NA
na_key <- c("NA"  = "-")
perf_measure_table  %<>%
  mutate_all(~ recode(.,!!!na_key))

# create output table
perf_measure_nice_table <-
  flextable(
    perf_measure_table,
    cheight = 0.2,
    cwidth = 1,
    defaults = list(fontname = "Calibri", font.size = 12)
  )

# Specificities and sensitivities for all classifier -----------------------------------------
# combine results
spec_sens_classifier <-
  cbind(
    results_opt_cutoffs,
    results_provided_cutoffs %>%
      dplyr::select(roche, igg, iga),
    results_rf,
    results_svm
  )

# select specificities and sensitivities
spec_sens_classifier <- spec_sens_classifier[2:3, ]

# assign names to the performance criteria
rownames(spec_sens_classifier) <- c("Specificity", "Sensitivity")

# transpose data frame
spec_sens_classifier %<>% t()

# rename classifier
rownames(spec_sens_classifier) <-
  c(
    "Roche N pan-Ig optimized cut-off",
    "Euroimmun S1 IgG optimized cut-off",
    "Euroimmun S1 IgA optimized cut-off",
    "Roche N pan-Ig manufacturer's cut-off",
    "Euroimmun S1 IgG manufacturer's cut-off",
    "Euroimmun S1 IgA manufacturer's cut-off",
    "Random Forest",
    "SVM"
  )

# save data frame
saveRDS(
  spec_sens_classifier,
  here_algo_results("specificity_sensitivity_classifier.RData")
)

# Output ------------------------------------------------------------------
# TABLE 1 in Olbrich et al. (2020)
docx_file <- here_out("cutoffs_and_performance.docx")
save_as_docx(perf_measure_nice_table, path = docx_file)


