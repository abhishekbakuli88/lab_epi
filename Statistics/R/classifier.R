## author: Peter PÃ¼tz, peter.puetz@uni-bielefeld.de

# This script creates the output table 1 in the lab paper (Olbrich et al., 2020),
# as well as cut-offs, unadjusted and adjusted prevalences as presented in the text and
# in Figure S3 in the epi paper (Pritsch et al. 2020)

# specify folders for script/data/output loading and saving
here_r = function (...)
  here::here("Statistics", "R", ...)
here_lab_data = function (...)
  here::here("Lab_paper2_data", ...)
here_algo_results = function (...)
  here::here("AlgorithmResults", ...)
here_out = function (...)
  here::here("Lab_paper2_output", "classifier", ...)
here_sampling_weights = function (...)
  here::here("SamplingWeights", ...)

# Create output folder if inexistent
dir.create(here_out(), showWarnings = FALSE)

# Load settings and functions
source(here_r("setup.R"))
source(here_r("functions.R"))

# Specify the list of required packages to be installed and load
Required_Packages <-
  c("tidyverse",
    "reshape2",
    "ggpubr",
    "flextable",
    "magrittr",
    "svglite")

# install (if necessary) and load packages
Install_And_Load(Required_Packages)

# Load data
#  Single data contains only one blood sample per participant
data = read.csv(here_lab_data("Global_Data", "Final_Lab_Single_20200904.csv"),
                stringsAsFactors = F) %>%
  as_tibble()

#  Unique labels
data[data == "Positiv" |
       data == "pos." | data == "reactive" |
       data == "positive"] = "positive"
data[data == "Negativ" |
       data == "Indeterminate" | data == "neg." | data == "ind." |
       data == "nonreactive" | data == "negative"] = "negative"

#  Remove false Roche 0 values (these are NAs)
val_cols = grep("roche_COI", colnames(data), value = T)
res_cols = grep("roche_Interpretation", colnames(data), value = T)

for (j in 1:length(val_cols)) {
  # Conveniently, the order is the same
  val_col = val_cols[[j]]
  res_col = res_cols[[j]]
  data[data[, val_col] == 0 & !is.na(data[, val_col]), res_col] = NA
  data[data[, val_col] == 0 & !is.na(data[, val_col]), val_col] = NA
}

# Extract last values for single tests
data = extract_last_values(data) %>% dplyr::rename("iga" = "Eur_IgA",
                                                   "igg" = "Eur_IgG",
                                                   "roche" = "Roche")

## performance criteria such as specificity for single tests based on
## manufacturers' cutoffs and for random forest
## and support vector machine (svm)
# either run respective script (takes some time)...
# source(here_r("algorithm_manucutoff_rf_svm.R"))
# ...or load data that is generated by this script, i.e.
# - performance criteria
results_provided_cutoffs <-
  readRDS(here_algo_results("results_provided_cutoffs.RData"))
results_rf <- readRDS(here_algo_results("results_rf.RData"))
results_svm <- readRDS(here_algo_results("results_svm.RData"))
# - "optimal" random forest and svm, will be used for predictions for cohort data
rf <- readRDS(here_algo_results("rf_optimal.RData"))
svmfit <- readRDS(here_algo_results("svm_optimal.RData"))

## optimised cut-offs and performance criteria such as specificity for single tests
## with optimized cutoffs
# either run respective script (takes some time)...
# source(here_r("algorithm_optcutoff.R"))
# ...or load data that is generated by this script, i.e.
# - performance criteria
results_opt_cutoffs <-
  readRDS(here_algo_results("perf_measures_primary.RData"))
results_opt_cutoffs_conf <-
  readRDS(here_algo_results("perf_measures_confirmatory.RData"))
# - optimized cutoffs
cutoffs_all_tests <-
  readRDS(here_algo_results("cutoffs.RData"))

# Performance measures for all classifiers -----------------------------------------
# combine performance measures
perf_measures <-
  cbind(results_opt_cutoffs, results_opt_cutoffs_conf) %>%
  # and transpose them
  t()

# select specificities, sensitivities and overall accuracy
perf_measures <- perf_measures[, 2:4]

# assign names to the performance criteria
colnames(perf_measures) <-
  c("Specificity", "Sensitivity", "Overall accuracy")

# combine with sample sizes and optimized cut-off
perf_measures %<>%
  as_tibble() %>%
  bind_cols(cutoffs_all_tests) %>%
  # select relevant columns
  dplyr::select(positives,
                negatives,
                Median,
                Specificity,
                Sensitivity,
                `Overall accuracy`) %>%
  # add Test variable
  add_column(Test = rownames(cutoffs_all_tests))


# transpose cutoffs and performance measures for manufacturers' cutoffs
results_provided_cutoffs_t <- results_provided_cutoffs %>%
  as_tibble() %>%
  t()

# assign colnames
colnames(results_provided_cutoffs_t) <-
  c(
    "Manufacturer's cut-off",
    "Spec. for Manu. cut-off",
    "Sens. for Manu. cut-off",
    "Ov. accur. for Manu cut-off",
    "au_roc"
  )

results_provided_cutoffs_final <- results_provided_cutoffs_t %>%
  as_tibble() %>%
  # add test variable
  add_column(Test = rownames(results_provided_cutoffs_t)) %>%
  # drop area under ROC
  select(-au_roc)

# add manufacturers' cutoffs and performance measures to table
perf_measure_table <-
  left_join(perf_measures, results_provided_cutoffs_final) %>%
  # drop some of the arrays
  dplyr::filter(!grepl("_IgA", Test),!grepl("_IgM", Test))

# add random forest and svm
ml_clas <- rbind(results_rf, results_svm)[, 1:4] %>%
  as_tibble()

# make colnames compatible
colnames(ml_clas) <-
  c("Median", "Specificity", "Sensitivity", "Overall accuracy")

# add sample sizes and make table compatible
ml_clas %<>%
  add_column(
    positives = c(193, 193),
    negatives = c(1073, 1073),
    Test = c("Random Forest", "SVM")
  )

# join with other classifiers
perf_measure_table %<>%
  bind_rows(ml_clas) %>%
  # reorder rows
  arrange(match(Test, c("iga", "igg", "roche"))) %>%
  # set to 3 decimals
  mutate_if(is.numeric,  ~ sprintf("%.4f", .))

# recode NA
na_key <- c("NA"  = "-")
perf_measure_table  %<>%
  mutate_all(~ recode(.,!!!na_key))

# create output table
perf_measure_nice_table <-
  flextable(
    perf_measure_table,
    cheight = 0.2,
    cwidth = 1,
    defaults = list(fontname = "Calibri", font.size = 12)
  )

# Specificities and sensitivities for all classifier -----------------------------------------
# combine results
spec_sens_classifier <-
  cbind(
    results_opt_cutoffs,
    results_provided_cutoffs %>%
      dplyr::select(roche, igg, iga),
    results_rf,
    results_svm
  )

# select specificities and sensitivities
spec_sens_classifier <- spec_sens_classifier[2:3, ]

# assign names to the performance criteria
rownames(spec_sens_classifier) <- c("Specificity", "Sensitivity")

# transpose data frame
spec_sens_classifier %<>% t()

# rename classifier
rownames(spec_sens_classifier) <-
  c(
    "Roche N pan-Ig optimized cut-off",
    "Euroimmun S1 IgG optimized cut-off",
    "Euroimmun S1 IgA optimized cut-off",
    "Roche N pan-Ig manufacturer's cut-off",
    "Euroimmun S1 IgG manufacturer's cut-off",
    "Euroimmun S1 IgA manufacturer's cut-off",
    "Random Forest",
    "SVM"
  )

# save data frame
saveRDS(
  spec_sens_classifier,
  here_algo_results("specificity_sensitivity_classifier.RData")
)


# prevalence adjustment ---------------------------------------------------

# read in unadjusted prevalences
prev_unadj <-
  read.csv(here_sampling_weights("Estimates.csv"), header = TRUE)

# only use weighted prevalences
prev_unadj %<>%
  as_tibble() %>%
  # filter(calculation == "weighted") %>%
  select(-calculation) %>%
  # divide by 100 to drop % units
  mutate_if(is.numeric,  ~ . / 100)

# add cut-offs
# extract cutoffs
cutoffs <- c(cutoff$new[c("Roche", "Eur_IgG", "Eur_IgA")],
             cutoff$old[c("Roche", "Eur_IgG", "Eur_IgA")])
# rename the tests
names(cutoffs) <-
  c("Roche", "IgG"  ,      "IgA",        "Roche_manu", "IgG_manu",   "IgA_manu")

# create data frame with the test names and the cutoffs
cutoffs_table <-
  tibble(test = names(cutoffs), `Cut-off` = unlist(cutoffs))

# merge with prevalence
prev_unadj <- left_join(prev_unadj, cutoffs_table) %>%
  # reorder columns
  dplyr::select(test, `Cut-off`, everything()) %>%
  # rename variables
  dplyr::rename(`Classifier` = test,
                `Seroprevalence` = estimate, )

# for renaming classifier, create character vector
rename_key <- rep(
  c(
    IgG  = "EI-S1-IgG optimized cut-off",
    IgA = "EI-S1-IgA optimized cut-off",
    Roche = "Ro-N-Ig optimized cut-off",
    IgG_manu  = "EI-S1-IgG manufacturer's cut-off",
    IgA_manu = "EI-S1-IgA manufacturer's cut-off",
    Roche_manu = "Ro-N-Ig manufacturer's cut-off",
    SVM = "Support Vector Machine",
    RF = "Random Forest"
  ),
  2
)

# rename tests
prev_unadj %<>%
  mutate(Classifier = recode(Classifier,!!!rename_key))

# store for calculations below (adjusted prevalences)
prev_unadj_help <- prev_unadj
# add dummy indicating weighting
prev_unadj_help %<>% add_column(weighted = c(rep("yes", nrow(prev_unadj_help) /
                                                   2),
                                             rep("no", nrow(prev_unadj_help) /
                                                   2)))


prev_unadj %<>%
  # set to 4 decimals
  mutate_if(is.numeric,  ~ sprintf("%.4f", .))

# store CI in one column
for (i in 1:nrow(prev_unadj))
  prev_unadj[i, "CI"] <-
  paste("[", prev_unadj[i, "lower_ci"], "; ", prev_unadj[i, "upper_ci"], "]", sep = "")

prev_unadj %<>%
  # drop irrelevant columns
  dplyr::select(-lower_ci,-upper_ci)

# recode NA
prev_unadj %<>%
  mutate_all(~ recode(.,!!!na_key))


# performance measures output ---------------------------------------------
# add Classifier variable to performance measures as created above
spec_sens_classifier %<>%
  as_tibble() %>%
  add_column(Classifier = rownames(spec_sens_classifier))

# rename tests
rename_key <- c(SVM = "Support Vector Machine",
                "Random Forest" = "Random Forest")
spec_sens_classifier %<>%
  mutate(Classifier = recode(Classifier,!!!rename_key)) %>%
  # reorder columns
  dplyr::select(Classifier, everything()) %>%
  # rename columns
  dplyr::rename("Our specificity" = Specificity,
                "Our sensitivity" = Sensitivity)

# add average specificity as well as low and high sensitivities as provided
# by the manufacturers
spec_sens_manu <-
  tibble(
    Classifier = c(
      "Roche N pan-Ig manufacturer's cut-off",
      "Euroimmun S1 IgG manufacturer's cut-off",
      "Euroimmun S1 IgA manufacturer's cut-off"
    ),
    `Manufacturer's specificity` = c(99.80 / 100,
                                     99.30 /
                                       100,
                                     92.40 /
                                       100),
    `Manufacturer's sensitivity (low)` = c(85.30 /
                                             100,
                                           87.50 / 100,
                                           91.67 / 100),
    `Manufacturer's sensitivity (high)` = c(99.50 /
                                              100,
                                            100 / 100,
                                            100 / 100)
  )

# merge with our performance measures
spec_sens_joint <- left_join(spec_sens_classifier, spec_sens_manu)

# rename classifiers
spec_sens_joint %<>% mutate(
  Classifier = str_replace(Classifier, "Euroimmun S1 IgG", "EI-S1-IgG"),
  Classifier = str_replace(Classifier, "Euroimmun S1 IgA", "EI-S1-IgA"),
  Classifier = str_replace(Classifier, "Roche N pan-Ig", "Ro-N-Ig")
)

# store for calculations below (adjusted prevalences)
spec_sens_joint_help <- spec_sens_joint


# prevalence adjusted -----------------------------------------------------
# first prevalences with our performance measures (spec. and sens.)
spec_sens_own <- spec_sens_joint_help %>%
  # drop manufacturers' performance measures
  dplyr::select(-contains("Manufact"))

# merge with unadjusted prevalences
unadj_own <- left_join(spec_sens_own, prev_unadj_help) %>%
  rename(spec = "Our specificity",
         sens = "Our sensitivity")

# now prevalences with manufacturers' performance measures (spec. and sens.)
spec_sens_manu <- spec_sens_joint_help %>%
  # drop rows with NAs for manufacturers' performance measures
  filter(!is.na(`Manufacturer's specificity`)) %>%
  # drop our performance measures
  select(-contains("Our"))

# reshape data such both sensitivities are attached to each specificity
spec_sens_manu %<>% gather(spec_sens, sens,
                           contains("sensitiv")) %>%
  rename(spec = "Manufacturer's specificity")

# merge with unadjusted prevalences
unadj_manu <- left_join(spec_sens_manu, prev_unadj_help)

# combine our and manufacturer data
unadj_merged <- unadj_own %>% bind_rows(unadj_manu)

# indicate if our spec and sens are used
unadj_merged %<>%
  mutate(
    spec_sens = ifelse(is.na(spec_sens), "Own", spec_sens),
    # and indicate whose cutoff is used
    `Cut-off` = ifelse(
      grepl("manufactu", Classifier),
      "Manufacturer",
      ifelse(grepl("optimized", Classifier), "Own", "-")
    )
  )

# calculate adjusted seroprevalence
adj_prev <- unadj_merged %>%
  mutate_at(c("Seroprevalence", "lower_ci", "upper_ci"),
            ~ (as.numeric(.) + spec - 1) / (sens + spec - 1)) %>%
  # replace negative values by 0
  mutate_at(c("Seroprevalence", "lower_ci", "upper_ci"),  ~ ifelse(. < 0, 0, .)) %>%
  # select relevant variables
  dplyr::select(Classifier,
                weighted,
                spec_sens,
                Seroprevalence,
                lower_ci,
                upper_ci)


# create dataset inlcuding all results
# merge unadjusted prevalences with performance measures
unadj_spec_sens <-
  left_join(prev_unadj_help, spec_sens_joint_help, "Classifier")

# merge resulting data frame with adjusted prevalences
final_data <-
  left_join(unadj_spec_sens, adj_prev, c("Classifier", "weighted")) %>%
  # sort
  arrange(Classifier, spec_sens) %>%
  # drop our specificity and sensitivity if it is not meaningful
  mutate_at(vars(starts_with("Our")),  ~ ifelse(spec_sens == "Own", ., NA)) %>%
  # drop manufacturer's specificity and sensitivity if it is not meaningful
  mutate_at(vars(contains("Manufacturer")),  ~ ifelse(spec_sens == "Own", NA, .)) %>%
  # reorder
  dplyr::select(Classifier, weighted, everything())

# adjust column names so that it can be distinguished between adjusted and unadjusted results
colnames(final_data) <-
  gsub("\\.x", "_unadjusted", colnames(final_data))
colnames(final_data) <-
  gsub("\\.y", "_adjusted", colnames(final_data))

# save dataset
saveRDS(final_data,
        here_algo_results("prevalence_estimates.RData"))

# Output ------------------------------------------------------------------

# TABLE 1 in Olbrich et al. (2020)
docx_file <- here_out("cutoffs_and_performance.docx")
save_as_docx(perf_measure_nice_table, path = docx_file)

# cut-offs, unadjusted and adjusted prevalences as presented in the text and
# in Figure S3 in the epi paper (Pritsch et al. 2020)
write.csv(final_data,
          here_algo_results("prevalence_estimates.csv"),
          row.names = FALSE)
